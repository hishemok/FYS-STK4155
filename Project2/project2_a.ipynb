{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d1eb5f-39ec-47ef-bb5b-00cd65986408",
   "metadata": {},
   "source": [
    "# Project 2 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6173aa7b-81c0-46bb-bc71-8bd71e6f0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229160e-e1f7-41ff-a07f-758632165020",
   "metadata": {},
   "source": [
    "## project 1 things -- not used so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87097aec-d302-4b3f-a008-07651ea92e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from project1 for ols\n",
    "\n",
    "import scipy.special # for binomial coefficient\n",
    "from sklearn.linear_model import LinearRegression , Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# not necc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad9f27ad-6bdb-4fa5-9bdc-e1fddf94fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' generate design matrix without intercept column for flattend x1, x2 '''\n",
    "def design_no_in(x1f, x2f, degreeplus1):\n",
    "    n = x1m.size\n",
    "    # 1. setting up design matrix\n",
    "    # we work here with intercept column\n",
    "    # the designmatrix s.t. we have the features 1, x1, x2, x1x2, x1^2, x2^2, ....\n",
    "    # with max totaldegree 5\n",
    "    # number of possible monoms up to degree 5 for 2 variables is 2+5 choose 5\n",
    "    num_of_monoms = scipy.special.binom(degreeplus1 + 1, degreeplus1 -1)\n",
    "    X = np.zeros((n, int(num_of_monoms))) # afterwards the intercept column is removed\n",
    "    c = 0\n",
    "    for i in range(degreeplus1):\n",
    "        for j in range(degreeplus1 - i):\n",
    "            X[:,c] = x1f**i * x2f**j\n",
    "            #print('{} {}'.format(i,j))\n",
    "            #print(X[:,c])\n",
    "            #print('x_1^{} * x_2^{}'.format(i,j)) \n",
    "            c += 1\n",
    "    # remove now intercept column\n",
    "    X = X[:,1:]\n",
    "    return X\n",
    "\n",
    "# for OLS k-fold\n",
    "def design_with_in(x1f, x2f, degreeplus1):\n",
    "    n = x1f.size\n",
    "    num_of_monoms = scipy.special.binom(degreeplus1 + 1, degreeplus1 -1)\n",
    "    X = np.zeros((n, int(num_of_monoms)))\n",
    "    c = 0\n",
    "    for i in range(degreeplus1):\n",
    "        for j in range(degreeplus1 - i):\n",
    "            X[:,c] = x1f**i * x2f**j\n",
    "            #print('{} {}'.format(i,j))\n",
    "            #print(X[:,c])\n",
    "            #print('x_1^{} * x_2^{}'.format(i,j)) \n",
    "            c += 1\n",
    "    return X\n",
    "\n",
    "'''\n",
    "generate data for meshgrid (x1m, x2m) and according y. degree is the maximum degree of the\n",
    "polynomial represented by the designmatrix\n",
    "'''\n",
    "def data(degree, x1m, x2m, y):\n",
    "    degreeplus1 = degree + 1\n",
    "\n",
    "    x1m_flat = x1m.flatten()\n",
    "    x2m_flat = x2m.flatten()\n",
    "    \n",
    "    X = design_no_in(x1m_flat, x2m_flat, degreeplus1)\n",
    "    \n",
    "    # 2. split the data in test and training data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y.flatten())\n",
    " \n",
    "    # 3. scaling of training data (by subtracting the mean value of each column)\n",
    "    X_train_mean = np.mean(X_train, axis=0)\n",
    "    X_train_scaled = X_train  - X_train_mean\n",
    "\n",
    "    y_train_mean = np.mean(y_train)\n",
    "    y_train_scaled = y_train - y_train_mean\n",
    "\n",
    "    # in order to use beta for test → test has to be scaled by training means\n",
    "    X_test_scaled = X_test - X_train_mean\n",
    "    #X_test_scaled = X_test - X_train_mean\n",
    "    y_test_scaled = y_test - y_train_mean\n",
    "\n",
    "    return X, X_train, X_test, X_train_scaled, X_train_mean, X_test_scaled, y_train_scaled, y_train_mean, y_train, y_test\n",
    "\n",
    "\n",
    "def OLS_analysis(maxdegree):\n",
    "    degrees = np.arange(1,maxdegree + 1)\n",
    "    num_monoms = int(scipy.special.binom(maxdegree + 2, maxdegree))\n",
    "\n",
    "    betas = np.zeros((maxdegree, num_monoms)) # for maxdegree = 5: 7 choose 5 is the number of possible monoms up to degree 5 for 2 variables (not -1 because the intercept will be added)\n",
    "    MSE = np.zeros((maxdegree,2)) # 0.column for train, 1. for test\n",
    "    R2 = np.zeros((maxdegree,2))\n",
    "    for degree in degrees:\n",
    "        print('DEGREE = ' + str(degree))\n",
    "        X, X_train, X_test, X_train_scaled, X_train_mean, X_test_scaled, y_train_scaled, y_train_mean, y_train, y_test = data(degree, x1m, x2m, y)\n",
    "        \n",
    "        beta_lin = np.linalg.inv(X_train_scaled.T @ X_train_scaled) @ X_train_scaled.T @ y_train_scaled # without intercept\n",
    "        intercept = y_train_mean - X_train_mean @ beta_lin\n",
    "        betas[degree-1] = np.hstack([intercept, beta_lin, np.zeros(( num_monoms -1 - beta_lin.size,))])\n",
    "    \n",
    "        #print('beta = {}'.format(betas[degree-1]))\n",
    "        \n",
    "        ypredict_train_LR =  X_train_scaled @ beta_lin + y_train_mean\n",
    "        ypredict_test_LR = X_test_scaled @ beta_lin + y_train_mean\n",
    "        ypredict_all_LR = X @ beta_lin + y_train_mean\n",
    "        \n",
    "        \n",
    "    \n",
    "        MSE[degree-1, 0] = mean_squared_error(y_train, ypredict_train_LR)\n",
    "        MSE[degree-1, 1] = mean_squared_error(y_test, ypredict_test_LR)\n",
    "        R2[degree-1, 0] = r2_score(y_train, ypredict_train_LR)\n",
    "        R2[degree-1, 1] = r2_score(y_test, ypredict_test_LR)\n",
    "\n",
    "        '''\n",
    "\n",
    "        print(\"Mean squared error for train: {}\".format(mean_squared_error(y_train, ypredict_train_LR)))\n",
    "        print('R^2 score for train:          {}'.format(r2_score(y_train, ypredict_train_LR)))\n",
    "        print(\"Mean squared error for test:  {}\".format(mean_squared_error(y_test, ypredict_test_LR)))\n",
    "        print('R^2 score for test:           {}'.format(r2_score(y_test, ypredict_test_LR)))\n",
    "            \n",
    "        print('summary statistics for train prediction LR')\n",
    "        print(pd.DataFrame(ypredict_train_LR).describe())\n",
    "        print('summary statistics for test prediction LR')\n",
    "        print(pd.DataFrame(ypredict_test_LR).describe())\n",
    "        '''\n",
    "\n",
    "    return MSE, R2, betas, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44fad4b3-3c92-4e82-ac18-7af7147fbcbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maxdegree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 56\u001b[0m\n\u001b[1;32m     49\u001b[0m         R2[degree\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m r2_score(y_test, ypredict_test_RR)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m betas, MSE, R2\n\u001b[0;32m---> 56\u001b[0m betas_lam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mint\u001b[39m(scipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mbinom(\u001b[43mmaxdegree\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, maxdegree)), num_lambdas))\n\u001b[1;32m     57\u001b[0m MSE_lam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m, num_lambdas))\n\u001b[1;32m     58\u001b[0m R2_lam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m, num_lambdas))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maxdegree' is not defined"
     ]
    }
   ],
   "source": [
    "def Ridge_analysis(lambd):\n",
    "\n",
    "    num_monoms = int(scipy.special.binom(maxdegree + 2, maxdegree))\n",
    "    betas = np.zeros((5, num_monoms)) # for maxdegree = 5: 7 choose 5 is the number of possible monoms up to degree 5 for 2 variables\n",
    "    MSE = np.zeros((5,2)) # 0.column for train, 1. for test\n",
    "    R2 = np.zeros((5,2))\n",
    "    \n",
    "    degrees = np.arange(1,maxdegree + 1)\n",
    "    for degree in degrees:\n",
    "        #print('DEGREE = ' + str(degree))\n",
    "        X, X_train, X_test, X_train_scaled, X_train_mean, X_test_scaled, y_train_scaled, y_train_mean, y_train, y_test = data(degree, x1m, x2m, y)\n",
    "        \n",
    "        # RIDGE REGRESSION:\n",
    "        beta_rr = np.linalg.inv(X_train_scaled.T @ X_train_scaled + lambd*np.identity(X_train_scaled.shape[1])) @ X_train_scaled.T @ y_train_scaled #without intercept\n",
    "        intercept = y_train_mean - X_train_mean @ beta_rr\n",
    "        betas[degree-1] = np.hstack([intercept,  beta_rr, np.zeros(( num_monoms  - 1 - beta_rr.size,))])\n",
    "                \n",
    "        #print('beta = {}'.format(beta_rr))\n",
    "        \n",
    "        ypredict_train_RR =  X_train_scaled @ beta_rr + y_train_mean\n",
    "        ypredict_test_RR = X_test_scaled @ beta_rr + y_train_mean\n",
    "        ypredict_all_RR = X @ beta_rr + y_train_mean\n",
    "\n",
    "        ''' for sake of understanding the functions of sklearn\n",
    "        # prediction using sklearn\n",
    "        RegRidge = Ridge(lambd)\n",
    "        RegRidge.fit(X_train,y_train)\n",
    "        ypredictRidge = RegRidge.predict(X_test)\n",
    "        print('difference of y_pred: true - sklearn, not scaled data (but also no intercept column): {}'.format(ypredict_test_RR - ypredictRidge))\n",
    "\n",
    "        ridge2 = Ridge(lambd, fit_intercept=False)\n",
    "        ridge2.fit(X_train_scaled,y_train_scaled)\n",
    "        y_pred2 = ridge2.predict(X_test_scaled) + y_train_mean\n",
    "        print('difference of y_pred: true - sklearn, scaled data (and also no intercept column): {}'.format(ypredict_test_RR - y_pred2))\n",
    "        print('difference of differences = {}'.format(ypredictRidge - y_pred2))\n",
    "        ''' \n",
    "\n",
    "    \n",
    "        '''\n",
    "        print(\"Mean squared error for train: {}\".format(mean_squared_error(y_train, ypredict_train_RR)))\n",
    "        print('R^2 score for train:          {}'.format(r2_score(y_train, ypredict_train_RR)))\n",
    "        print(\"Mean squared error for test:  {}\".format(mean_squared_error(y_test, ypredict_test_RR)))\n",
    "        print('R^2 score for test:           {}'.format(r2_score(y_test, ypredict_test_RR)))\n",
    "        '''\n",
    "        \n",
    "        MSE[degree-1, 0] = mean_squared_error(y_train, ypredict_train_RR)\n",
    "        MSE[degree-1, 1] = mean_squared_error(y_test, ypredict_test_RR)\n",
    "        R2[degree-1, 0] = r2_score(y_train, ypredict_train_RR)\n",
    "        R2[degree-1, 1] = r2_score(y_test, ypredict_test_RR)\n",
    "\n",
    "\n",
    "    return betas, MSE, R2\n",
    "\n",
    "\n",
    "\n",
    "betas_lam = np.zeros((5, int(scipy.special.binom(maxdegree + 2, maxdegree)), num_lambdas))\n",
    "MSE_lam = np.zeros((5,2, num_lambdas))\n",
    "R2_lam = np.zeros((5,2, num_lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d7176-462b-47b5-ad46-c5d47f45d158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee81c975-96d4-46ff-b14a-e27b8e2e9b7f",
   "metadata": {},
   "source": [
    "## week 41 - exercise functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485efcd-e093-4edc-b471-f5130ce2b02e",
   "metadata": {},
   "source": [
    "I consider the function $f(x) = 1 - 2x + 1/3x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "baa29fc0-5584-46cc-a0e3-6dbefd98aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n = 100\n",
    "# Make data set.\n",
    "x = np.linspace(-1,6, n).reshape(-1, 1)\n",
    "y = 1 - 2*x +1/3*x**2\n",
    "\n",
    "X = np.c_[np.ones((n,1)),x, x**2]\n",
    "#yr = 1 - 2*x +1/3*x**2 + (np.random.rand(n,1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba98da41-f45a-40c2-9c68-2036fcbbe477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y)\n",
    "#plt.plot(x,yr,'o')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baacd2f-eb75-4e3b-b13b-1575b7c0e44e",
   "metadata": {},
   "source": [
    "### Gradient descent / steepest descent\n",
    "First we consider OLS regression, and thus the function that we want to minimize is $$C(\\beta) = 1/n \\| X\\beta - y \\|^2_2 = 1/n (X\\beta - y)^\\top (X\\beta - y) = 1/n \\beta^\\top X^\\top X \\beta - 2/n \\beta^\\top X^\\top y + 1/n y^\\top y$$\n",
    "So we know that the gradient w.r.t $\\beta$ is $$\\nabla C(\\beta) = 2/n X^\\top (X\\beta - y)$$\n",
    "Since the second derivative is $2/n X^\\top X$, which is positive semidefinite, we know that $C(\\beta)$ is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cddd29d-9f07-4d0e-b258-993e7e55c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_C_MSE(X,y,beta, method='OLS', lamb=0):\n",
    "    n = y.size\n",
    "    if method == 'OLS':\n",
    "        return 2.0/n * X.T @(X@ beta -y)\n",
    "    if method == 'RIDGE':\n",
    "        return 2.0/n * X.T @(X@ beta -y) + 2.0* lamb*beta\n",
    "    \n",
    "    \n",
    "\n",
    "''' simple gradient descent'''\n",
    "''' if gamma = 0 → the optimum \n",
    "    gamma by hessian is chosen, else the gamma value'''\n",
    "def gd_gamma(X,y,f_gradient, niter=1000, gamma=0, method='OLS', lamb=0):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "        \n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "\n",
    "    if not gamma:\n",
    "        # considering optimum stepsize wrt deeplearning\n",
    "        if method == 'OLS':\n",
    "            H = (2.0/n)* X.T @ X\n",
    "        if method =='RIDGE':\n",
    "            H = (2.0/n)* X.T @ X + 2.0*lamb\n",
    "\n",
    "        # Get the eigenvalues\n",
    "        EigValues, EigVectors = np.linalg.eig(H)\n",
    "        gamma = 1.0/np.max(EigValues) # optimum stepssize wrt to deeplearningbook.org page 86\n",
    "    \n",
    "    for iter in range(niter):\n",
    "        xk -= gamma*f_gradient(X,y,xk, method, lamb)\n",
    "    beta_GD = xk\n",
    "    return beta_GD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bad7f-53f4-4067-82b0-a46fadbb088d",
   "metadata": {},
   "source": [
    "### plain GD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14a58ba4-83b0-4f42-ba3c-b4166730f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momentum(X,y,f_grad, method='OLS', lamb=0, Niter=1000, gamma=0, moment=0.3):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "    \n",
    "    if not gamma:\n",
    "        # considering optimum stepsize wrt deeplearning\n",
    "        if method == 'OLS':\n",
    "            H = (2.0/n)* X.T @ X\n",
    "        if method =='RIDGE':\n",
    "            H = (2.0/n)* X.T @ X + 2.0*lamb\n",
    "\n",
    "        # Get the eigenvalues\n",
    "        EigValues, EigVectors = np.linalg.eig(H)\n",
    "        gamma = 1.0/np.max(EigValues) # optimum stepssize wrt to deeplearningbook.org page 86\n",
    "\n",
    "    \n",
    "    change=0.0\n",
    "    \n",
    "    for iter in range(Niter):\n",
    "        change = gamma*f_gradient(X,y,xk, method, lamb) + moment*change\n",
    "        xk -= change\n",
    "    beta_GD = xk\n",
    "    return beta_GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9a1fd-4307-43d7-9d23-8e0a2b3002fa",
   "metadata": {},
   "source": [
    "### stochastic GD with time decay scaling (or gama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbe98713-8b8d-486e-b926-5aa1d013f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' OLD which  I GUESS is incorrect, but somehow worked in lectures with this ''' \n",
    "def sum_c_i(X_sub, beta, y_sub,n, method='OLS', lamb=0): # Xsub is a X from which rows are deleted, same for y_sub\n",
    "    if method == 'OLS':\n",
    "        return X_sub.T @ ((X_sub @ beta) - y_sub) *2/n\n",
    "    if method == 'RIDGE':\n",
    "        return X_sub.T @ ((X_sub @ beta) - y_sub) *2/n  + 2*lamb*y_sub.size/n\n",
    "\n",
    "\n",
    "''' What I think would be more correct \n",
    "# Assuming that we do not minimize MSE but RSS for OLS and RSS+ lambbeta^Tbeta\n",
    "def sum_c_i(X_sub, beta, y_sub,n, method='OLS', lamb=0): # Xsub is a X from which rows are deleted, same for y_sub\n",
    "    if method == 'OLS':\n",
    "        return X_sub.T @ ((X_sub @ beta) - y_sub) *2\n",
    "    if method == 'RIDGE':\n",
    "        return X_sub.T @ ((X_sub @ beta) - y_sub) *2  + 2*lamb\n",
    "\n",
    "    #TODO IWIE MACHEN ALLE DIESES AVERAGINING MIT 1/M ABER KA WIESO\n",
    "'''\n",
    "def gamma_timedecay(t,t0,t1):\n",
    "    if t0 <= 0 or t1 <= 0:\n",
    "        raise ValueError(\"t0 or t1 <= 0\")\n",
    "    return t0/(t+t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76401e4b-5b0e-4f74-9721-017935fdc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X,y,sum_c_i, method='OLS',lamb=0, n_epochs=50, M=5, t0=1,t1=10, gam = 0):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "    n = y.size\n",
    "    if M > n:\n",
    "        raise ValueError(\"can't take Minimbatch of size %i from sest of size %i\" %(M,n))\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "\n",
    "\n",
    "    m = int(y.size/M) #number of minibatches\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        shuff = np.random.choice(X.shape[0], X.shape[0], False)\n",
    "        X_shuff = X[shuff]\n",
    "        y_shuff = y[shuff]\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m) \n",
    "            Xi = X_shuff[random_index:random_index+M]\n",
    "            yi = y_shuff[random_index:random_index+M]\n",
    "            gradient = sum_c_i(Xi, xk, yi,n, method, lamb) # 1/M\n",
    "            if gam:\n",
    "                gamma = gam\n",
    "\n",
    "            else: \n",
    "                gamma = gamma_timedecay(e*m + i,t0,t1)\n",
    "            xk = xk - gamma*gradient\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f1b79-2189-493f-86a0-928123724226",
   "metadata": {},
   "source": [
    "### stochastic gd + time decay scaling (or gamma) + momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbb0295b-4867-4bbe-b454-42cfc79b2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note by setting momentum = 0, we have the old sgd\n",
    "def sgd_momentum(X,y,sum_c_i, method='OLS', lamb=0, n_epochs=50, M=5, t0=1,t1=10, gam=0, momentum = 0.9):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "    n = y.size\n",
    "    if M > n:\n",
    "        raise ValueError(\"can't take Minimbatch of size %i from sest of size %i\" %(M,n))\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "\n",
    "    if not gam:\n",
    "        gamma_f = gamma_timedecay\n",
    "    else:\n",
    "        gamma_f = lambda x, y, z: gam\n",
    "\n",
    "    m = int(y.size/M) #number of minibatches\n",
    "    change = 0\n",
    "    for e in range(n_epochs):\n",
    "        shuff = np.random.choice(X.shape[0], X.shape[0], False)\n",
    "        X_shuff = X[shuff]\n",
    "        y_shuff = y[shuff]\n",
    "\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m) \n",
    "            Xi = X_shuff[random_index:random_index+M]\n",
    "            yi = y_shuff[random_index:random_index+M]\n",
    "            gradient =  sum_c_i(Xi, xk, yi,n, method, lamb)\n",
    "\n",
    "            gamma = gamma_f(e*m + i,t0,t1)\n",
    "            change = gamma*gradient + change*momentum\n",
    "            xk = xk - change\n",
    "\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39349f-df0e-4726-8f2d-a56ee9e112e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d480ad7-d888-4f85-ab4f-1839680b6d4a",
   "metadata": {},
   "source": [
    "### adagrad with stochastic gd with & without momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f3c3ff3-2451-4d07-bca2-175f18ae83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum_adagrad(X,y,sum_c_i, method='OLS', lamb=0, n_epochs=50, M=5, gamma=0.001, momentum = 0.9, delta=1e-8):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "    n = y.size\n",
    "    if M > n:\n",
    "        raise ValueError(\"can't take Minibatch of size %i from sest of size %i\" %(M,n))\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "\n",
    "    m = int(y.size/M) #number of minibatches\n",
    "    change = 0.0\n",
    "    for e in range(n_epochs):\n",
    "        shuff = np.random.choice(X.shape[0], X.shape[0], False)\n",
    "        X_shuff = X[shuff]\n",
    "        y_shuff = y[shuff]\n",
    "\n",
    "        Giter = 0.0\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m) \n",
    "            Xi = X_shuff[random_index:random_index+M]\n",
    "            yi = y_shuff[random_index:random_index+M]\n",
    "            gradient =  sum_c_i(Xi, xk, yi,n,method,lamb)\n",
    "            Giter += gradient*gradient\n",
    "            change = gamma*gradient/(delta + np.sqrt(Giter)) + change*momentum\n",
    "            xk = xk - change\n",
    "\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfe239-9670-4ef5-b59f-6f1b4be8cc7e",
   "metadata": {},
   "source": [
    "### RMS propagation (Stochastic gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36b716bd-63b7-40ff-bdf4-eac0091987b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_rmsprop(X,y,sum_c_i, method='OLS', lamb=0, n_epochs=50, M=5, gamma=0.001, momentum = 0.9, delta=1e-8):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "\n",
    "    n = y.size\n",
    "    if M > n:\n",
    "        raise ValueError(\"can't take Minimbatch of size %i from sest of size %i\" %(M,n))\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "\n",
    "\n",
    "    m = int(y.size/M) #number of minibatches\n",
    "    change = 0.0\n",
    "    for e in range(n_epochs):\n",
    "        shuff = np.random.choice(X.shape[0], X.shape[0], False)\n",
    "        X_shuff = X[shuff]\n",
    "        y_shuff = y[shuff]\n",
    "\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m) \n",
    "            Xi = X_shuff[random_index:random_index+M]\n",
    "            yi = y_shuff[random_index:random_index+M]\n",
    "            gradient =  sum_c_i(Xi, xk, yi,n, method, lamb)\n",
    "            Giter = gradient*gradient\n",
    "            change = (1- momentum)*Giter + change*momentum\n",
    "            xk = xk - gamma*gradient/(np.sqrt(change) + delta)\n",
    "\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf88981-0558-4f53-958e-db3df53f416d",
   "metadata": {},
   "source": [
    "### ADAM stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3a940f1-29bf-4a5a-830f-81e8803fbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_adam(X,y,sum_c_i, method='OLS', lamb=0, n_epochs=50, M=5, gamma=0.001, momentum = 0.9, secmomentum = 0.99, delta=1e-8):\n",
    "    if method not in ['OLS', 'RIDGE']:\n",
    "        raise ValueError('no valid method chosen.')\n",
    "\n",
    "    n = y.size\n",
    "    if M > n:\n",
    "        raise ValueError(\"can't take Minimbatch of size %i from sest of size %i\" %(M,n))\n",
    "    deg = X.shape[1]\n",
    "    xk = np.random.randn(deg,1) # starting point for beta\n",
    "    m = int(y.size/M) #number of minibatches\n",
    "    \n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        shuff = np.random.choice(X.shape[0], X.shape[0], False)\n",
    "        X_shuff = X[shuff]\n",
    "        y_shuff = y[shuff]\n",
    "\n",
    "        mk = 0.0\n",
    "        sk= 0.0\n",
    "        for k in range(1, m+1):\n",
    "            random_index = M*np.random.randint(m) \n",
    "            Xi = X_shuff[random_index:random_index+M]\n",
    "            yi = y_shuff[random_index:random_index+M]\n",
    "            gradient =  sum_c_i(Xi, xk, yi,n, method, lamb)\n",
    "            Giter = gradient*gradient\n",
    "            mk = ((1- momentum)*gradient + momentum*mk)/(1- momentum**k)\n",
    "            sk = ((1- secmomentum)*Giter + secmomentum*sk)/(1-secmomentum**k)\n",
    "            xk = xk - gamma*mk/(np.sqrt(sk) + delta)\n",
    "\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747099c-1677-45c6-8dd3-6032fd64c5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e167ed-2e52-4b54-9b28-41b8c322e5ca",
   "metadata": {},
   "source": [
    "## replace analytical gradient using autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe543cf-c967-46e2-979c-0780137da044",
   "metadata": {},
   "source": [
    "For the non stochastic variants we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9412827d-2806-4098-9f70-386aee30f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE for OLS and MSE+ lambda * beta^Tbeta for RIDGE ( but this is not the TRUE criterion we have for ridge minimiation !!!!!)\n",
    "def cost(X,y,beta, method='OLS',lamb=0):\n",
    "    if method == 'OLS':\n",
    "        return np.sum((y-X @ beta)**2)/ X.shape[0]    #(X@ beta -y).T @ (X @ beta - y) \n",
    "    elif method == 'RIDGE':\n",
    "        return np.sum((y-X @ beta)**2)/ X.shape[0] + np.sum(beta**2) #(X@ beta -y).T @ (X @ beta - y) + lamb * beta.T @ beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34a8a6-4d8b-4c30-9aa1-a9c289f756a3",
   "metadata": {},
   "source": [
    "And then the gradient with respect to beta is given by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6d9d089-a574-4f46-a48e-bb717acbe4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158.5],\n",
       "       [237. ],\n",
       "       [261.5],\n",
       "       [227. ]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient function\n",
    "cost_grad = grad(cost, 2)\n",
    "\n",
    "# e.g.\n",
    "X = np.array([[1,1,1,1], [1,2,3,4], [2,4,4,2], [3,3,3,3]])\n",
    "y = np.array([1,2,1,1])\n",
    "beta = np.array([0.5, 1, 2, 1]).reshape(-1,1)\n",
    "lamb = 10\n",
    "method = 'RIDGE'\n",
    "#method = 'OLS'\n",
    "cost_grad(X,y,beta, method, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef639ae7-de08-4489-87ef-7c01ec155fd7",
   "metadata": {},
   "source": [
    "And for the stochastic gradients consider the following:\n",
    "\n",
    "In order to get $\\nabla C(\\beta) = \\sum_{i=1}^n \\nabla c_i(x_i, \\beta)$\n",
    "\n",
    "For OLS (with lambda = 0) and RIDGE this $c_i(x_i)$ is simply $$\n",
    "\\frac{1}{n}(x_i \\beta - y_i)^2 + \\frac{1}{n}\\lambda  \\sum_{j=1}^p \\beta_j^2$$\n",
    "So the sum of some $c_i(x_i)$ for $i \\in I$ is $$\\sum_{i \\in I} c_i(x_i) = \\frac{1}{n} \\Big(\\sum_{i \\in I} (x_i \\beta - y_i)^2 \\Big) + \\frac{|I|}{n}\\lambda  \\sum_{j=1}^p \\beta_j^2 = \\frac{1}{n} \\Big(  |I| \\lambda  \\sum_{j=1}^p \\beta_j^2 +   \\sum_{i \\in I} (x_i \\beta - y_i)^2\\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8b2ff66-ebc0-459a-bbc3-ceac49e71b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that if X_sub = X ... then have cost\n",
    "def subset_cost(X_sub,y_sub,beta, n, method='OLS', lamb=0):\n",
    "    if method == 'OLS':\n",
    "        return np.sum((y_sub-X_sub @ beta)**2)/ n  #(X@ beta -y).T @ (X @ beta - y) \n",
    "    elif method == 'RIDGE':\n",
    "        return 1/n * (np.sum((y_sub-X_sub @ beta)**2) + X_sub.shape[0] * np.sum(beta**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff05a8ac-52e8-4cf5-a649-51587d380cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158.5],\n",
       "       [237. ],\n",
       "       [261.5],\n",
       "       [227. ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient function\n",
    "subset_cost_grad = grad(subset_cost, 2)\n",
    "\n",
    "# e.g. \n",
    "n = X.shape[0]\n",
    "subset_cost_grad(X,y,beta,n, method, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd73e0b-65bf-4815-bd62-3136c51c358b",
   "metadata": {},
   "source": [
    "SO, when using the above gradient descnet functions, the derivative function that one gives into the function can be introduced by this autograd derivatives using the cost & the subset_cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8446e3-829f-46a5-a449-e987bae46ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629c8a0e-d443-4e7c-af5d-2da4dbd7c446",
   "metadata": {},
   "source": [
    "## Franke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "087012be-9364-45cb-952a-916317402ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankeFunction(x,y, noise=0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4 + noise*np.random.normal(0, 1, (n,n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d87141-e58a-4954-b52c-aaee9f8f9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x1 = np.linspace(0, 1, n)\n",
    "x2 = np.linspace(0, 1, n)\n",
    "x1m, x2m = np.meshgrid(x1,x2)\n",
    "y = FrankeFunction(x1m, x2m, noise=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63786ed-aaca-45ea-9873-1465739aef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(x1m, x2m, y, cmap='viridis')\n",
    "# ax.plot_surface(xm, ym, z, cmap='viridis')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "#add colorbarz_tilde_ridge.reshape(100,100)\n",
    "plt.colorbar(surf)\n",
    "plt.title('Franke function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5d909-1d2b-40c4-8135-f965f939b3cc",
   "metadata": {},
   "source": [
    "TODO: Analsyis fro franke,\n",
    "I just did an anlysis for a simpler 1-dim function in week41"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sklearn-env",
   "language": "python",
   "name": ".sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
